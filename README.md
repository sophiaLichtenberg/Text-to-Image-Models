# ğŸŒŸAwesome TEXT-TO-IMAGE Models
<div align="center">
  <img src="logo.png" alt="Logo" width="300">
  <h1 align="center">Overview</h1>
  
[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![](https://img.shields.io/github/last-commit/sophiaLichtenberg/Text-to-Image-Models)  

</div>


<div align="center">
  <p style="font-size:20px;"><em>A curated collection of resources focused on Bias in Text-to-Image Models.</em></p>
</div>


## Table of Contents
- [Surveys](#-surveys)
- [Papers](#-papers)
  - [Cultural Bias](#-cultural-bias)
  - [Social Bias](#-social-bias)
- [Conferences](#-Conferences)




<!--- 
## ğŸ“š Surveys:
([Back to Table of Contents](#Table-of-Contents))
+ [A Survey on Sparse Autoencoders: Interpreting the Internal Mechanisms of Large Language Models](https://arxiv.org/abs/2503.05613) (Jun. 06, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/pdf/2503.05613)
  
+ [A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models](https://arxiv.org/abs/2502.17516) (Feb. 22, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2502.17516)

+ [Open Problems in Mechanistic Interpretability](https://arxiv.org/abs/2501.16496) (Jan. 27, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.16496)

+ [A Review of Multimodal Explainable Artificial Intelligence: Past, Present and Future](https://arxiv.org/abs/2412.14056) (Dec. 18, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.14056)

+ [Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey](https://arxiv.org/abs/2412.02104) (Dec. 3, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2412.02104)


## ğŸ“š Blog:
([Back to Table of Contents](#table-of-contents))
+ [Mechanistic Interpretability Meets Vision Language Models: Insights and Limitations](https://d2jud02ci9yv69.cloudfront.net/2025-04-28-vlm-understanding-29/blog/vlm-understanding/) (Apr. 28, 2025)

+ [Are SAE features from the Base Model still meaningful to LLaVA?](https://www.lesswrong.com/posts/8JTi7N3nQmjoRRuMD/are-sae-features-from-the-base-model-still-meaningful-to-1) (Dec. 6, 2024)

+ [Bridging the VLM and mech interp communities for multimodal interpretability](https://www.lesswrong.com/posts/aa5fzGr8JA3pqvhYC/bridging-the-vlm-and-mech-interp-communities-for-multimodal) (Oct. 28, 2024)

+ [Case Study: Interpreting, Manipulating, and Controlling CLIP With Sparse Autoencoders](https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip) (Aug. 2, 2025)

+ [Interpreting and Steering Features in Images](https://www.lesswrong.com/posts/Quqekpvx8BGMMcaem/interpreting-and-steering-features-in-images) (Apr. 28, 2025)

+ [Case Study: Interpreting, Manipulating, and Controlling CLIP With Sparse Autoencoders](https://www.lesswrong.com/posts/iYFuZo9BMvr6GgMs5/case-study-interpreting-manipulating-and-controlling-clip) (Apr. 28, 2025)

+ [Laying the Foundations for Vision and Multimodal Mechanistic Interpretability & Open Problems](https://www.lesswrong.com/posts/kobJymvvcvhbjWFKe/laying-the-foundations-for-vision-and-multimodal-mechanistic) (May. 14, 2024)

+ [Towards Multimodal Interpretability: Learning Sparse Interpretable Features in Vision Transformers](https://www.lesswrong.com/posts/bCtbuWraqYTDtuARg/towards-multimodal-interpretability-learning-sparse-2) (Apr. 30, 2024)

-->



## ğŸ“š Papers:



### ğŸ“œ Cultural Bias
([Back to Table of Contents](#table-of-contents))


+ **TIBET** [TIBET: Identifying and Evaluating Biases in Text-to-Image Generative Models](https://arxiv.org/abs/2312.01261) (Dec. 3, 2023)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2312.01261)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://tibet-ai.github.io/)
  
+ **KITTEN** [KITTEN: A Knowledge-Intensive Evaluation of Image Generation on Visual Entities](https://arxiv.org/abs/2410.11824) (Oct. 15, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.11824)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://kitten-project.github.io/)

+ **CUBE** [Beyond Aesthetics: Cultural Competence in Text-to-Image Models](https://arxiv.org/abs/2407.06863) (Jul. 9, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.06863)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://github.com/google-deepmind/cube)

+ **WISE** [WISE: A World Knowledge-Informed Semantic Evaluation for Text-to-Image Generation](https://arxiv.org/abs/2503.07265) (Mar. 10, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.07265)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://github.com/PKU-YuanGroup/WISE)
  
+ **TIGeR** [TIGeR: Unifying Text-to-Image Generation and Retrieval with Large Multimodal Models](https://arxiv.org/abs/2406.05814) (Jun. 9, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2406.05814)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://tiger-t2i.github.io/)
  
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ“œ Social Bias
([Back to Table of Contents](#table-of-contents))


+ **FAIntbench** [FAIntbench: A Holistic and Precise Benchmark for Bias Evaluation in Text-to-Image Models](https://arxiv.org/abs/2405.17814) (May. 28, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2405.17814)
  [![Star](https://img.shields.io/github/stars/Astarojth/FAIntbench-v1.svg?style=social&label=Star)](https://github.com/Astarojth/FAIntbench-v1)

+ **OASIS** [OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes](https://arxiv.org/abs/2501.00962) (May. 7, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2501.00962)

+ **Bias Connect** [Mitigate One, Skew Another? Tackling Intersectional Biases in Text-to-Image Models](https://arxiv.org/abs/2505.17280) (May. 22, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2505.17280)


+ **Bias Connect** [BiasConnect: Investigating Bias Interactions in Text-to-Image Models](https://arxiv.org/abs/2503.09763) (Mar. 12, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.09763)


  
+ **BIGbench** [BIGbench: A Unified Benchmark for Evaluating Multi-dimensional Social Biases in Text-to-Image Models](https://arxiv.org/abs/2407.15240) (Jul. 21, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2407.15240)
  [![Star](https://img.shields.io/github/stars/BIGbench2024/BIGbench2024.svg?style=social&label=Star)](https://github.com/BIGbench2024/BIGbench2024)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------



### ğŸ—“ï¸ Conferences 
([Back to Table of Contents](#table-of-contents))

| Conference | Year | Abstract Deadline | Full Paper Deadline | Location      |
|------------|:----:|:----------------:|:-----------------:|---------------|
| NeurIPS    | 2026 | May              | May               | ...           |
| ICML       | 2026 | Jan              | Jan               | Seoul         |
| AIES       | 2026 | May              | May               | ...           |
| ECCV       | 2026 | Feb              | Mar               | MalmÃ¶         |
| WACV       | 2026 | Jul/Sep          | Jul/Sep           | Tucson        |
| CVPR       | 2026 | Nov              | Nov               | Denver        |
| ICCV       | 2027 | Mar              | Mar               | Hong Kong     |
| WACV       | 2027 | Jul/Sep          | Jul/Sep           | ...           |
| CVPR       | 2027 | Nov              | Nov               | Seattle       |
| ECCV       | 2028 | Feb              | Mar               | ...           |
| WACV       | 2028 | Jul/Sep          | Jul/Sep           | ...           |
| CVPR       | 2028 | Nov              | Nov               | San Antonio   |
| ICCV       | 2029 | Mar              | Mar               | Dubai         |
| WACV       | 2029 | Jul/Sep          | Jul/Sep           | ...           |
| CVPR       | 2029 | Nov              | Nov               | Los Angeles   |



-------------------------------------------------------------------------------------------------------------------------------------------------------------------------



### ğŸ—“ï¸ Timeline 
([Back to Table of Contents](#table-of-contents))

### PhD Monthly Timetable (Transposed)

| Paper / Month       | Aug 2025 | Sep 2025 | Oct 2025 | Nov 2025 | Dec 2025 | Jan 2026 | Feb 2026 | Mar 2026 | Apr 2026 | May 2026 | Jun 2026 | Jul 2026 | Aug 2026 | Sep 2026 | Oct 2026 | Nov 2026 | Dec 2026 | Jan 2027 | Feb â€“ Mar 2027 | Apr â€“ Jun 2027 | Jul â€“ Nov 2027 | Dec 2027 â€“ Mar 2028 | Apr â€“ Nov 2028 | Dec 2028 â€“ Jan 2029 |
|--------------------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------|----------------|----------------|----------------|--------------------|----------------|--------------------|
| **Paper 1 (ICML 2026)** | ğŸ”¹ Lit & experiments | ğŸ”¹ Experiments | ğŸ”¹ Write intro | ğŸ”¹ Draft paper | ğŸ”¹ Submit ICML |          |          |          |          |          |          |          |          |          |          |          |          |          |                |                |                |                    |                |                    |
| **Paper 2 (CVPR 2026)** |          |          |          |          |          | ğŸ”¹ Idea generation | ğŸ”¹ Start experiments | ğŸ”¹ Continue experiments | ğŸ”¹ Draft sections | ğŸ”¹ Write results | ğŸ”¹ Draft paper | ğŸ”¹ Internal review | ğŸ”¹ Final revisions | ğŸ”¹ Submit CVPR |          |          |          |          |                |                | ğŸ”¹ Draft & submit NeurIPS/CVPR 2027 |                    | ğŸ”¹ Draft & submit CVPR 2028 |                    |
| **Paper 3 (ICCV 2027)** |          |          |          |          |          |          |          | ğŸ”¹ Idea planning |          |          | ğŸ”¹ Start experiments | ğŸ”¹ Continue experiments | ğŸ”¹ Continue experiments | ğŸ”¹ Continue experiments | ğŸ”¹ Draft full paper | ğŸ”¹ Internal review & revisions | ğŸ”¹ Submit ICCV | ğŸ”¹ Follow up reviews | ğŸ”¹ Additional experiments / new project ideas |                |                |                    |                |                    |
| **Paper 4/5 (Optional)** |          |          |          |          |          |          |          |          |          |          |          |          |          |          |          |          |          |          |                | ğŸ”¹ Paper 4 experiments | ğŸ”¹ Draft & submit NeurIPS/CVPR 2027 | ğŸ”¹ ECCV 2028 planning & experiments | ğŸ”¹ Draft & submit CVPR 2028 | ğŸ”¹ ICCV 2029 submission & thesis wrap-up |








<!--- ### ğŸ“œ Input-level Attribution
([Back to Table of Contents](#table-of-contents))
+ **Token Activation Map for VLLM** [Token Activation Map to Visually Explain Multimodal LLMs](https://arxiv.org/abs/2411.16198) (Apr. 4, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2506.23270)
  [![Star](https://img.shields.io/github/stars/xmed-lab/TAM.svg?style=social&label=Star)](https://github.com/xmed-lab/TAM)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------

### ğŸ“œ Sparse Autoencoder
([Back to Table of Contents](#table-of-contents))

+ **DiffLens: Dissecting and Mitigating Diffusion Bias** [Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability](https://arxiv.org/abs/2503.20483) (Mar. 26, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.20483)
  [![Star](https://img.shields.io/github/stars/foundation-model-research/DiffLens.svg?style=social&label=Star)](https://github.com/foundation-model-research/DiffLens)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
### ğŸ“œ Beyond or Logit Lens
([Back to Table of Contents](#table-of-contents))

+ **Diffusion Steering Lens Decoding ViTs** [Decoding Vision Transformers: the Diffusion Steering Lens](https://arxiv.org/abs/2504.13763) (Apr. 23, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.13763)
  [![Star](https://img.shields.io/github/stars/rtakatsky/DSLens.svg?style=social&label=Star)](https://github.com/rtakatsky/DSLens)

+ **Diffusion Lens** [Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines](https://arxiv.org/abs/2403.05846) (Oct. 21, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2403.05846)
  [![Star](https://img.shields.io/github/stars/tokeron/DiffusionLens.svg?style=social&label=Star)](https://github.com/tokeron/DiffusionLens)

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------
### ğŸ“œ Steering 
([Back to Table of Contents](#table-of-contents))
+ **Diffusion Steering Lens Decoding ViTs** [Decoding Vision Transformers: the Diffusion Steering Lens](https://arxiv.org/abs/2504.13763) (Apr. 23, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.13763)
  [![Star](https://img.shields.io/github/stars/rtakatsky/DSLens.svg?style=social&label=Star)](https://github.com/rtakatsky/DSLens)

+ **DiffLens: Dissecting and Mitigating Diffusion Bias** [Dissecting and Mitigating Diffusion Bias via Mechanistic Interpretability](https://arxiv.org/abs/2503.20483) (Mar. 26, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2503.20483)
  [![Star](https://img.shields.io/github/stars/foundation-model-research/DiffLens.svg?style=social&label=Star)](https://github.com/foundation-model-research/DiffLens)
  
+ **Concept Sliders** [Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models](https://link.springer.com/chapter/10.1007/978-3-031-73661-2_10) (Nov. 10, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://link.springer.com/chapter/10.1007/978-3-031-73661-2_10)
  [![Star](https://img.shields.io/github/stars/CompVis/attribute-conl.svg?style=social&label=Star)](https://sliders.baulab.info/)

## ğŸ“š Tool
([Back to Table of Contents](#table-of-contents))

+ **Prisma** [Prisma : An Open Source Toolkit for Mechanistic Interpretability in Vision and Video](https://arxiv.org/abs/2504.19475) (Apr. 28, 2025)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2504.19475)
  [![Star](https://img.shields.io/github/stars/Prisma-Multimodal/ViT-Prisma.svg?style=social&label=Star)](https://github.com/Prisma-Multimodal/ViT-Prisma)

+ **LLaVA-Intrepret** [Towards Interpreting Visual Information Processing in Vision-Language Models](https://arxiv.org/abs/2410.07149) (Oct. 09, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2410.07149)
  [![Star](https://img.shields.io/github/stars/clemneo/llava-interp.svg?style=social&label=Star)](https://github.com/clemneo/llava-interp)

+ **LVLM-Intrepret** [LVLM-Intrepret: An Interpretability Tool for Large Vision-Language Models](https://arxiv.org/abs/2404.03118) (June. 24, 2024)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2404.03118)
  [![Star](https://img.shields.io/github/stars/IntelLabs/lvlm-interpret.svg?style=social&label=Star)](https://github.com/IntelLabs/lvlm-interpret)

+ **ViT Prisma** [ViT Prisma: A Mechanistic Interpretability Library for Vision Transformers](https://github.com/soniajoseph/vit-prisma) (2023)
  [![Star](https://img.shields.io/github/stars/soniajoseph/vit-prisma.svg?style=social&label=Star)](https://github.com/soniajoseph/vit-prisma)

+ **Causal Tracing Tool 4 BLIP** [Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP](https://arxiv.org/abs/2308.14179) (Aug. 27, 2023)
  [![arXiv](https://img.shields.io/badge/arXiv-b31b1b.svg)](https://arxiv.org/abs/2308.14179)
  [![Star](https://img.shields.io/github/stars/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.svg?style=social&label=Star)](https://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability)



-->
